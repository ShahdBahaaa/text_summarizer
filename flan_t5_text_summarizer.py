# -*- coding: utf-8 -*-
"""flan_t5_text_summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1liWA9NKfPqEL7NqkWY91ol7-H52cks-Z
"""

!pip install -q transformers datasets sentencepiece accelerate rouge-score

import torch
from datasets import load_dataset
from transformers import (
    T5Tokenizer,
    T5ForConditionalGeneration,
    DataCollatorForSeq2Seq,
    Trainer,
    TrainingArguments
)

from rouge_score import rouge_scorer

dataset = load_dataset("cnn_dailymail", "3.0.0", trust_remote_code=True)

# dataset = load_dataset("samsum")

model_name = "google/flan-t5-base"  # use "small" if GPU is weak
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def preprocess(batch):
    inputs = ["summarize: " + doc for doc in batch["article"]]
    targets = batch["highlights"]

    model_inputs = tokenizer(
        inputs,
        truncation=True,
        padding="max_length",
        max_length=512
    )

    labels = tokenizer(
        targets,
        truncation=True,
        padding="max_length",
        max_length=128
    )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

small_train = dataset["train"].shuffle(seed=42).select(range(500))
small_val   = dataset["validation"].shuffle(seed=42).select(range(100))

tokenized_train = small_train.map(preprocess, batched=True, remove_columns=small_train.column_names)
tokenized_val   = small_val.map(preprocess, batched=True, remove_columns=small_val.column_names)

training_args = TrainingArguments(
    output_dir="./flan_t5_summarizer",
    eval_strategy="steps",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    num_train_epochs=3,
    logging_steps=50,
    save_steps=500,
    eval_steps=500,
    save_total_limit=2,
    fp16=torch.cuda.is_available(),
    report_to="none"
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# ===== BASE MODEL (BEFORE FINE-TUNING) =====
base_model = T5ForConditionalGeneration.from_pretrained(
    "google/flan-t5-base"
).to(device)

def summarize_base(text):
    inputs = tokenizer(
        "summarize: " + text,
        return_tensors="pt",
        truncation=True,
        max_length=512
    ).to(device)

    outputs = base_model.generate(
        **inputs,
        max_length=120
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

trainer.train()

def summarize(text):
    input_text = "summarize: " + text
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512).to(device)

    outputs = model.generate(
        **inputs,
        max_length=120,
        num_beams=4,
        early_stopping=True
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

scorer = rouge_scorer.RougeScorer(
    ["rouge1", "rouge2", "rougeL"],
    use_stemmer=True
)

def rouge_scores(reference, generated):
    scores = scorer.score(reference, generated)
    return {
        "ROUGE-1": scores["rouge1"].fmeasure,
        "ROUGE-2": scores["rouge2"].fmeasure,
        "ROUGE-L": scores["rougeL"].fmeasure
    }

sample = dataset["test"][0]

generated = summarize(sample["article"])
reference = sample["highlights"]

print("GENERATED SUMMARY:\n", generated)
print("\nREFERENCE SUMMARY:\n", reference)
print("\nROUGE:\n", rouge_scores(reference, generated))

model.save_pretrained("./flan_t5_finetuned")
tokenizer.save_pretrained("./flan_t5_finetuned")

